{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weber50432/miniconda3/envs/ML/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import torch_geometric\n",
    "from datasets.BatchWSI import BatchWSI\n",
    "from models.model_graph_mil import *\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "dataroot = '/home/weber50432/UCL/final_dissertation/test/graph_files/'\n",
    "large_graph_pt = 'TCGA-BH-A0C1-01Z-00-DX1.21FE357E-B182-4397-BFEF-7E96E994236A.pt' # example input\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Data Structure\n",
    "- `N`: number of patches\n",
    "- `M`: number of edges\n",
    "- `centroid`: [N x 2] matrix containing centroids for each patch\n",
    "- `edge_index`: [2 x M] matrix containing edges between patches (connected via adjacent spatial coordinates)\n",
    "- `edge_latent`: [2 x M] matric containing edges between patches (connected via latent space)\n",
    "- `x`: [N x 1024] matrix which uses 1024-dim extracted ResNet features for each iamge patch (features saved for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[42430, 1024], edge_index=[2, 339440], edge_latent=[2, 339440], centroid=[42430, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(os.path.join(dataroot, large_graph_pt))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     1,     1],\n",
      "        [27479, 29109, 31371, 31605, 36505, 38065, 10195, 13223, 27479, 29109]])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1],\n",
      "        [ 1,  2,  5,  3,  6, 17, 18,  7,  0,  2]])\n"
     ]
    }
   ],
   "source": [
    "print(data.edge_latent[:, :10])\n",
    "print(data.edge_index[:, :10])  # 顯示前10條 edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch Geometric, inference on large graphs is very tractable. Here, adjacency matrices are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs), and node and target features are simply concatenated in the node dimension\n",
    "\n",
    "This procedure has some crucial advantages over other batching procedures:\n",
    "\n",
    "- GNN operators that rely on a message passing scheme do not need to be modified since messages still cannot be exchanged between two nodes that belong to different graphs.\n",
    "\n",
    "- There is no computational or memory overhead. For example, this batching procedure works completely without any padding of node or edge features. Note that there is no additional memory overhead for adjacency matrices since they are saved in a sparse fashion holding only non-zero entries, i.e., the edges. \n",
    "- For more details, see the advanced mini-batching FAQ in: https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mBatchWSI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlarge_graph_pt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlarge_graph_pt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data\n",
      "File \u001b[0;32m~/UCL/final_dissertation/Patch-GCN/datasets/BatchWSI.py:129\u001b[0m, in \u001b[0;36mBatchWSI.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys, update_cat_dims)\u001b[0m\n\u001b[1;32m    126\u001b[0m batch\u001b[38;5;241m.\u001b[39m__num_nodes_list__ \u001b[38;5;241m=\u001b[39m num_nodes_list\n\u001b[1;32m    128\u001b[0m ref_data \u001b[38;5;241m=\u001b[39m data_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[1;32m    130\u001b[0m     items \u001b[38;5;241m=\u001b[39m batch[key]\n\u001b[1;32m    131\u001b[0m     item \u001b[38;5;241m=\u001b[39m items[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "data = BatchWSI.from_data_list([torch.load(os.path.join(dataroot, large_graph_pt)), \n",
    "                                torch.load(os.path.join(dataroot, large_graph_pt))])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference + Backprop using 23K patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BatchWSI.from_data_list([torch.load(os.path.join(dataroot, large_graph_pt))])\n",
    "data = data.to(device)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'num_layers': 4, 'edge_agg': 'spatial', 'resample': 0, 'n_classes': 1}\n",
    "model = PatchGCN_Surv(**model_dict).to(device)\n",
    "print(\"Number of Parameters:\", count_parameters(model))\n",
    "\n",
    "### Example Forward Paas + Gradient Backprop\n",
    "start = time.time()\n",
    "out = model(x_path=data)\n",
    "out[0].backward()\n",
    "print('Time Elapsed: %0.5f seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference + Backprop using 92K patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulating a very large graph (containing 4 subgraphs of 23K patches each)\n",
    "data = BatchWSI.from_data_list([torch.load(os.path.join(dataroot, large_graph_pt)), \n",
    "                                torch.load(os.path.join(dataroot, large_graph_pt)),\n",
    "                                torch.load(os.path.join(dataroot, large_graph_pt)),\n",
    "                                torch.load(os.path.join(dataroot, large_graph_pt))])\n",
    "data = data.to(device)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'num_layers': 4, 'edge_agg': 'spatial', 'resample': 0, 'n_classes': 1}\n",
    "model = PatchGCN_Surv(**model_dict).to(device)\n",
    "print(\"Number of Parameters:\", count_parameters(model))\n",
    "\n",
    "### Example Forward Paas + Gradient Backprop\n",
    "start = time.time()\n",
    "out = model(x_path=data)\n",
    "out[0].backward()\n",
    "print('Time Elapsed: %0.5f seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming worst case scenario that every graph has ~100K patches, for a dataset of 1000 WSIs, an epoch would take 3.43 minutes, with 20 epochs taking ~ 1 hour."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
